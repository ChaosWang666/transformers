#!/usr/bin/env python3
"""
æµ‹è¯•ä¿®æ”¹åçš„Qwen2.5-VLæ¨¡å‹ä¸torch.compileçš„å…¼å®¹æ€§

ä¸»è¦æµ‹è¯•ç‚¹ï¼š
1. ç¼–è¯‘åçš„æ¨¡å‹æ˜¯å¦æ­£å¸¸è¿è¡Œ
2. ä½ç½®ç¼–ç ä¼˜åŒ–æ˜¯å¦æ­£ç¡®
3. KV Cacheæœºåˆ¶æ˜¯å¦å…¼å®¹
"""

import torch
import sys
import os
sys.path.insert(0, os.path.abspath("./src"))

def test_torch_compile_compatibility():
    """æµ‹è¯•torch.compileå…¼å®¹æ€§"""
    
    # æ£€æŸ¥æ˜¯å¦æ”¯æŒtorch.compile
    if not hasattr(torch, 'compile'):
        print("å½“å‰ç¯å¢ƒä¸æ”¯æŒtorch.compileï¼Œè·³è¿‡æµ‹è¯•")
        return
        
    try:
        from transformers.models.qwen2_5_vl.modular_qwen2_5_vl import (
            Qwen2_5_VLForConditionalGeneration, 
            Qwen2_5_VLConfig
        )
    except ImportError as e:
        print(f"å¯¼å…¥å¤±è´¥: {e}")
        return
        
    print("å¼€å§‹torch.compileå…¼å®¹æ€§æµ‹è¯•...")
    
    # åˆ›å»ºæœ€å°é…ç½®
    config = Qwen2_5_VLConfig(
        hidden_size=64,
        vocab_size=1000,
        num_hidden_layers=2,
        num_attention_heads=2,
        max_position_embeddings=512,
    )
    
    try:
        # åˆ›å»ºæ¨¡å‹å®ä¾‹
        model = Qwen2_5_VLForConditionalGeneration._from_config(config)
        model.eval()
        
        # ç¼–è¯‘æ¨¡å‹
        print("æ­£åœ¨ç¼–è¯‘æ¨¡å‹...")
        compiled_model = torch.compile(model)
        
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        batch_size = 2
        seq_length = 10
        input_ids = torch.randint(0, 100, (batch_size, seq_length))
        attention_mask = torch.ones_like(input_ids)
        
        print("æµ‹è¯•ç¼–è¯‘åçš„æ¨¡å‹å‰å‘ä¼ æ’­...")
        
        # æµ‹è¯•é¢„å¡«å……é˜¶æ®µ
        with torch.no_grad():
            outputs = compiled_model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                use_cache=True
            )
            
        print(f"âœ“ é¢„å¡«å……é˜¶æ®µæˆåŠŸï¼Œè¾“å‡ºlogitså½¢çŠ¶: {outputs.logits.shape}")
        print(f"âœ“ è¿”å›KVç¼“å­˜: {outputs.past_key_values is not None}")
        
        # æµ‹è¯•ç”Ÿæˆé˜¶æ®µï¼ˆä½¿ç”¨KVç¼“å­˜ï¼‰
        next_token_ids = torch.randint(0, 100, (batch_size, 1))
        
        with torch.no_grad():
            outputs2 = compiled_model(
                input_ids=next_token_ids,
                attention_mask=torch.cat([attention_mask, torch.ones((batch_size, 1))], dim=1),
                past_key_values=outputs.past_key_values,
                use_cache=True
            )
            
        print(f"âœ“ ç”Ÿæˆé˜¶æ®µæˆåŠŸï¼Œè¾“å‡ºlogitså½¢çŠ¶: {outputs2.logits.shape}")
        
        print("ğŸ‰ torch.compileå…¼å®¹æ€§æµ‹è¯•é€šè¿‡ï¼")
        
    except Exception as e:
        print(f"âŒ torch.compileå…¼å®¹æ€§æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def test_position_encoding_consistency():
    """æµ‹è¯•ä½ç½®ç¼–ç çš„ä¸€è‡´æ€§"""
    
    try:
        from transformers.models.qwen2_5_vl.modular_qwen2_5_vl import (
            Qwen2_5_VLForConditionalGeneration, 
            Qwen2_5_VLConfig
        )
    except ImportError as e:
        print(f"å¯¼å…¥å¤±è´¥: {e}")
        return
        
    print("å¼€å§‹ä½ç½®ç¼–ç ä¸€è‡´æ€§æµ‹è¯•...")
    
    # åˆ›å»ºé…ç½®
    config = Qwen2_5_VLConfig(
        hidden_size=64,
        vocab_size=1000,
        num_hidden_layers=2,
        num_attention_heads=2,
        max_position_embeddings=512,
    )
    
    try:
        model = Qwen2_5_VLForConditionalGeneration._from_config(config)
        model.eval()
        
        batch_size = 2
        seq_length = 8
        input_ids = torch.randint(0, 100, (batch_size, seq_length))
        attention_mask = torch.ones_like(input_ids)
        
        # æµ‹è¯•prepare_inputs_for_generation
        print("æµ‹è¯•prepare_inputs_for_generation...")
        
        with torch.no_grad():
            # é¢„å¡«å……é˜¶æ®µ
            prepared_inputs = model.prepare_inputs_for_generation(
                input_ids=input_ids,
                attention_mask=attention_mask,
                use_cache=True,
                cache_position=None,
                past_key_values=None
            )
            
            print(f"âœ“ é¢„å¡«å……é˜¶æ®µposition_idså½¢çŠ¶: {prepared_inputs.get('position_ids', 'None')}")
            
            # æ¨¡æ‹Ÿç¬¬ä¸€æ¬¡å‰å‘ä¼ æ’­
            outputs = model(**prepared_inputs)
            
            # ç”Ÿæˆé˜¶æ®µ
            next_input_ids = torch.randint(0, 100, (batch_size, 1))
            new_attention_mask = torch.cat([attention_mask, torch.ones((batch_size, 1))], dim=1)
            
            prepared_inputs2 = model.prepare_inputs_for_generation(
                input_ids=next_input_ids,
                attention_mask=new_attention_mask,
                past_key_values=outputs.past_key_values,
                cache_position=torch.tensor([seq_length]),
                use_cache=True
            )
            
            position_ids = prepared_inputs2.get('position_ids')
            if position_ids is not None:
                print(f"âœ“ ç”Ÿæˆé˜¶æ®µposition_idså½¢çŠ¶: {position_ids.shape}")
                print(f"âœ“ position_idsè®¾å¤‡: {position_ids.device}")
            else:
                print("âš  ç”Ÿæˆé˜¶æ®µæœªè¿”å›position_ids")
            
        print("ğŸ‰ ä½ç½®ç¼–ç ä¸€è‡´æ€§æµ‹è¯•é€šè¿‡ï¼")
        
    except Exception as e:
        print(f"âŒ ä½ç½®ç¼–ç ä¸€è‡´æ€§æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    print("=== Qwen2.5-VL ä¼˜åŒ–ä»£ç å…¼å®¹æ€§æµ‹è¯• ===")
    print()
    
    test_position_encoding_consistency()
    print()
    test_torch_compile_compatibility()
    print()
    print("æµ‹è¯•å®Œæˆï¼")