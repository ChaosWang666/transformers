#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨
#           This file was automatically generated from src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_qwen2_5_vl.py file directly. One of our CI enforces this.
#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨
# coding=utf-8
"""
Qwen2.5-VL æ¨¡å‹é…ç½®æ–‡ä»¶

æœ¬æ–‡ä»¶åŒ…å« Qwen2.5-VL å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„é…ç½®ç±»å®šä¹‰ã€‚Qwen2.5-VL æ˜¯ä¸€ä¸ªæ”¯æŒå›¾åƒã€è§†é¢‘å’Œæ–‡æœ¬
ç†è§£çš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œå…·æœ‰ä»¥ä¸‹ä¸»è¦ç‰¹ç‚¹ï¼š

1. å¤šæ¨¡æ€æ¶æ„ï¼š
   - è§†è§‰ç¼–ç å™¨ï¼šå¤„ç†å›¾åƒå’Œè§†é¢‘è¾“å…¥ï¼Œæ”¯æŒä»»æ„åˆ†è¾¨ç‡
   - æ–‡æœ¬ç¼–ç å™¨ï¼šåŸºäº Qwen2 æ¶æ„çš„è¯­è¨€æ¨¡å‹
   - å¤šæ¨¡æ€èåˆï¼šå°†è§†è§‰ç‰¹å¾åµŒå…¥åˆ°æ–‡æœ¬åºåˆ—ä¸­

2. é…ç½®å±‚æ¬¡ç»“æ„ï¼š
   - Qwen2_5_VLVisionConfigï¼šè§†è§‰ç¼–ç å™¨é…ç½®
   - Qwen2_5_VLTextConfigï¼šæ–‡æœ¬ç¼–ç å™¨é…ç½®  
   - Qwen2_5_VLConfigï¼šé¡¶å±‚é…ç½®ï¼Œæ•´åˆè§†è§‰å’Œæ–‡æœ¬é…ç½®

3. æ ¸å¿ƒåŠŸèƒ½ï¼š
   - æ”¯æŒå›¾åƒç†è§£å’Œæè¿°
   - æ”¯æŒè§†é¢‘å†…å®¹åˆ†æ
   - æ”¯æŒå¤šè½®å¯¹è¯
   - æ”¯æŒæŒ‡ä»¤è·Ÿéš

è¯¥é…ç½®ç³»ç»Ÿä¸ºæ¨¡å‹çš„åˆå§‹åŒ–ã€è®­ç»ƒå’Œæ¨ç†æä¾›äº†çµæ´»çš„å‚æ•°ç®¡ç†æœºåˆ¶ã€‚
"""
# Copyright 2025 The Qwen Team and The HuggingFace Inc. team. All rights reserved.
#
# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX
# and OPT implementations in this library. It has been modified from its
# original forms to accommodate minor architectural differences compared
# to GPT-NeoX and OPT used by the Meta AI team that trained the model.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from ...configuration_utils import PretrainedConfig, layer_type_validation
from ...modeling_rope_utils import rope_config_validation


class Qwen2_5_VLVisionConfig(PretrainedConfig):
    """
    Qwen2.5-VL è§†è§‰ç¼–ç å™¨é…ç½®ç±»ã€‚
    
    ç”¨äºé…ç½®è§†è§‰ Transformer çš„å„ç§å‚æ•°ï¼ŒåŒ…æ‹¬ç½‘ç»œæ·±åº¦ã€éšè—å±‚å¤§å°ã€æ³¨æ„åŠ›å¤´æ•°ç­‰ã€‚
    è¯¥é…ç½®ç±»å®šä¹‰äº†è§†è§‰ç¼–ç å™¨å¤„ç†å›¾åƒå’Œè§†é¢‘è¾“å…¥æ—¶çš„æ‰€æœ‰å…³é”®å‚æ•°ã€‚
    
    Args:
        depth (int, optional): Transformer å±‚æ•°ï¼Œé»˜è®¤ 32ã€‚æ§åˆ¶è§†è§‰ç¼–ç å™¨çš„æ·±åº¦ã€‚
        hidden_size (int, optional): éšè—å±‚ç»´åº¦ï¼Œé»˜è®¤ 3584ã€‚å®šä¹‰æ¯å±‚çš„ç‰¹å¾ç»´åº¦ã€‚
        hidden_act (str, optional): æ¿€æ´»å‡½æ•°ç±»å‹ï¼Œé»˜è®¤ "silu"ã€‚ç”¨äº MLP å±‚çš„éçº¿æ€§æ¿€æ´»ã€‚
        intermediate_size (int, optional): MLP ä¸­é—´å±‚ç»´åº¦ï¼Œé»˜è®¤ 3420ã€‚å‰é¦ˆç½‘ç»œçš„ä¸­é—´å±‚å¤§å°ã€‚
        num_heads (int, optional): æ³¨æ„åŠ›å¤´æ•°ï¼Œé»˜è®¤ 16ã€‚å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶çš„å¤´æ•°ã€‚
        in_channels (int, optional): è¾“å…¥å›¾åƒé€šé“æ•°ï¼Œé»˜è®¤ 3 (RGB)ã€‚
        patch_size (int, optional): å›¾åƒå—å¤§å°ï¼Œé»˜è®¤ 14x14ã€‚å°†å›¾åƒåˆ†å‰²æˆå—çš„å°ºå¯¸ã€‚
        spatial_merge_size (int, optional): ç©ºé—´åˆå¹¶å¤§å°ï¼Œé»˜è®¤ 2x2ã€‚ç”¨äºå‡å°‘åºåˆ—é•¿åº¦ã€‚
        temporal_patch_size (int, optional): æ—¶é—´ç»´åº¦å—å¤§å°ï¼Œé»˜è®¤ 2ã€‚è§†é¢‘å¸§çš„æ—¶é—´åˆ†ç»„ã€‚
        tokens_per_second (int, optional): æ¯ç§’ token æ•°ï¼Œé»˜è®¤ 4ã€‚è§†é¢‘å¤„ç†çš„æ—¶é—´åˆ†è¾¨ç‡ã€‚
        window_size (int, optional): çª—å£å¤§å°ï¼Œé»˜è®¤ 112ã€‚çª—å£æ³¨æ„åŠ›çš„çª—å£å°ºå¯¸ã€‚
        out_hidden_size (int, optional): è¾“å‡ºéšè—å±‚ç»´åº¦ï¼Œé»˜è®¤ 3584ã€‚æœ€ç»ˆè¾“å‡ºçš„ç‰¹å¾ç»´åº¦ã€‚
        fullatt_block_indexes (list, optional): ä½¿ç”¨å…¨æ³¨æ„åŠ›çš„å±‚ç´¢å¼•ï¼Œé»˜è®¤ [7, 15, 23, 31]ã€‚
        initializer_range (float, optional): å‚æ•°åˆå§‹åŒ–èŒƒå›´ï¼Œé»˜è®¤ 0.02ã€‚æƒé‡åˆå§‹åŒ–çš„æ ‡å‡†å·®ã€‚
    """
    model_type = "qwen2_5_vl"
    base_config_key = "vision_config"

    def __init__(
        self,
        depth=32,
        hidden_size=3584,
        hidden_act="silu",
        intermediate_size=3420,
        num_heads=16,
        in_channels=3,
        patch_size=14,
        spatial_merge_size=2,
        temporal_patch_size=2,
        tokens_per_second=4,
        window_size=112,
        out_hidden_size=3584,
        fullatt_block_indexes=[7, 15, 23, 31],
        initializer_range=0.02,
        **kwargs,
    ):
        super().__init__(**kwargs)

        # ç½‘ç»œç»“æ„å‚æ•°
        self.depth = depth  # Transformer å±‚æ•°
        self.hidden_size = hidden_size  # éšè—å±‚ç»´åº¦
        self.hidden_act = hidden_act  # æ¿€æ´»å‡½æ•°ç±»å‹
        self.intermediate_size = intermediate_size  # MLP ä¸­é—´å±‚ç»´åº¦
        self.num_heads = num_heads  # æ³¨æ„åŠ›å¤´æ•°
        
        # è¾“å…¥å¤„ç†å‚æ•°
        self.in_channels = in_channels  # è¾“å…¥é€šé“æ•°ï¼ˆRGB=3ï¼‰
        self.patch_size = patch_size  # å›¾åƒå—å¤§å°
        self.spatial_merge_size = spatial_merge_size  # ç©ºé—´åˆå¹¶å¤§å°
        self.temporal_patch_size = temporal_patch_size  # æ—¶é—´ç»´åº¦å—å¤§å°
        
        # è§†é¢‘å¤„ç†å‚æ•°
        self.tokens_per_second = tokens_per_second  # æ¯ç§’ token æ•°
        
        # æ³¨æ„åŠ›æœºåˆ¶å‚æ•°
        self.window_size = window_size  # çª—å£æ³¨æ„åŠ›å¤§å°
        self.fullatt_block_indexes = fullatt_block_indexes  # å…¨æ³¨æ„åŠ›å±‚ç´¢å¼•
        
        # è¾“å‡ºå’Œåˆå§‹åŒ–å‚æ•°
        self.out_hidden_size = out_hidden_size  # è¾“å‡ºéšè—å±‚ç»´åº¦
        self.initializer_range = initializer_range  # å‚æ•°åˆå§‹åŒ–èŒƒå›´


class Qwen2_5_VLTextConfig(PretrainedConfig):
    r"""
    Qwen2.5-VL æ–‡æœ¬ç¼–ç å™¨é…ç½®ç±»ã€‚
    
    ç”¨äºå­˜å‚¨ Qwen2_5_VLTextModel çš„é…ç½®ä¿¡æ¯ã€‚è¯¥é…ç½®ç±»ç”¨äºæ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–
    Qwen2.5-VL æ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å‚æ•°å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äº
    Qwen2-VL-7B-Instruct [Qwen/Qwen2-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct) çš„é…ç½®ã€‚

    é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª [`PretrainedConfig`]ï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚æ›´å¤šä¿¡æ¯è¯·å‚é˜…
    [`PretrainedConfig`] çš„æ–‡æ¡£ã€‚
    
    è¯¥é…ç½®ç±»å®šä¹‰äº† Qwen2.5-VL æ¨¡å‹ä¸­æ–‡æœ¬ç¼–ç å™¨éƒ¨åˆ†çš„æ‰€æœ‰å…³é”®å‚æ•°ï¼ŒåŒ…æ‹¬ï¼š
    - è¯æ±‡è¡¨å’ŒåµŒå…¥å‚æ•°
    - Transformer æ¶æ„å‚æ•°ï¼ˆå±‚æ•°ã€æ³¨æ„åŠ›å¤´æ•°ç­‰ï¼‰
    - ä½ç½®ç¼–ç å’Œæ³¨æ„åŠ›æœºåˆ¶é…ç½®
    - å¤šæ¨¡æ€ token å¤„ç†å‚æ•°

    Args:
        vocab_size (`int`, *optional*, defaults to 152064):
            è¯æ±‡è¡¨å¤§å°ã€‚å®šä¹‰äº†ä¼ é€’ç»™ [`Qwen2_5_VLModel`] çš„ `inputs_ids` å¯ä»¥è¡¨ç¤ºçš„ä¸åŒ token æ•°é‡ã€‚
        hidden_size (`int`, *optional*, defaults to 8192):
            éšè—è¡¨ç¤ºçš„ç»´åº¦ã€‚è¿™æ˜¯æ¨¡å‹å†…éƒ¨ç‰¹å¾å‘é‡çš„å¤§å°ã€‚
        intermediate_size (`int`, *optional*, defaults to 29568):
            MLPï¼ˆå¤šå±‚æ„ŸçŸ¥æœºï¼‰è¡¨ç¤ºçš„ç»´åº¦ã€‚å‰é¦ˆç½‘ç»œä¸­é—´å±‚çš„å¤§å°ã€‚
        num_hidden_layers (`int`, *optional*, defaults to 80):
            Transformer ç¼–ç å™¨ä¸­çš„éšè—å±‚æ•°é‡ã€‚æ§åˆ¶æ¨¡å‹çš„æ·±åº¦ã€‚
        num_attention_heads (`int`, *optional*, defaults to 64):
            Transformer ç¼–ç å™¨ä¸­æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¤´æ•°é‡ã€‚
        num_key_value_heads (`int`, *optional*, defaults to 8):
            ç”¨äºå®ç°åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆGrouped Query Attentionï¼‰çš„é”®å€¼å¤´æ•°é‡ã€‚å¦‚æœ
            `num_key_value_heads=num_attention_heads`ï¼Œæ¨¡å‹å°†ä½¿ç”¨å¤šå¤´æ³¨æ„åŠ›ï¼ˆMHAï¼‰ï¼›å¦‚æœ
            `num_key_value_heads=1`ï¼Œæ¨¡å‹å°†ä½¿ç”¨å¤šæŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆMQAï¼‰ï¼›å¦åˆ™ä½¿ç”¨ GQAã€‚
            å°†å¤šå¤´æ£€æŸ¥ç‚¹è½¬æ¢ä¸º GQA æ£€æŸ¥ç‚¹æ—¶ï¼Œæ¯ä¸ªç»„çš„é”®å€¼å¤´åº”é€šè¿‡å¯¹è¯¥ç»„å†…æ‰€æœ‰åŸå§‹å¤´è¿›è¡Œå‡å€¼æ± åŒ–æ¥æ„å»ºã€‚
            æ›´å¤šè¯¦æƒ…è¯·å‚é˜…[æ­¤è®ºæ–‡](https://huggingface.co/papers/2305.13245)ã€‚å¦‚æœæœªæŒ‡å®šï¼Œé»˜è®¤ä¸º `32`ã€‚
        hidden_act (`str` or `function`, *optional*, defaults to `"silu"`):
            è§£ç å™¨ä¸­çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå‡½æ•°æˆ–å­—ç¬¦ä¸²ï¼‰ã€‚
        max_position_embeddings (`int`, *optional*, defaults to 32768):
            æ­¤æ¨¡å‹å¯èƒ½ä½¿ç”¨çš„æœ€å¤§åºåˆ—é•¿åº¦ã€‚å®šä¹‰ä½ç½®ç¼–ç çš„èŒƒå›´ã€‚
        initializer_range (`float`, *optional*, defaults to 0.02):
            ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„æˆªæ–­æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–å™¨çš„æ ‡å‡†å·®ã€‚
        rms_norm_eps (`float`, *optional*, defaults to 1e-05):
            RMS å½’ä¸€åŒ–å±‚ä½¿ç”¨çš„ epsilon å€¼ã€‚ç”¨äºæ•°å€¼ç¨³å®šæ€§ã€‚
        use_cache (`bool`, *optional*, defaults to `True`):
            æ¨¡å‹æ˜¯å¦åº”è¿”å›æœ€åçš„é”®/å€¼æ³¨æ„åŠ›ï¼ˆå¹¶éæ‰€æœ‰æ¨¡å‹éƒ½ä½¿ç”¨ï¼‰ã€‚ä»…åœ¨ `config.is_decoder=True` æ—¶ç›¸å…³ã€‚
        tie_word_embeddings (`bool`, *optional*, defaults to `False`):
            æ¨¡å‹çš„è¾“å…¥å’Œè¾“å‡ºè¯åµŒå…¥æ˜¯å¦åº”è¯¥ç»‘å®šï¼ˆå…±äº«æƒé‡ï¼‰ã€‚
        rope_theta (`float`, *optional*, defaults to 1000000.0):
            RoPEï¼ˆæ—‹è½¬ä½ç½®ç¼–ç ï¼‰åµŒå…¥çš„åŸºç¡€å‘¨æœŸã€‚æ§åˆ¶ä½ç½®ç¼–ç çš„é¢‘ç‡ç‰¹æ€§ã€‚
        use_sliding_window (`bool`, *optional*, defaults to `False`):
            æ˜¯å¦ä½¿ç”¨æ»‘åŠ¨çª—å£æ³¨æ„åŠ›ã€‚å¯ä»¥å‡å°‘é•¿åºåˆ—çš„è®¡ç®—å¤æ‚åº¦ã€‚
        sliding_window (`int`, *optional*, defaults to 4096):
            æ»‘åŠ¨çª—å£æ³¨æ„åŠ›ï¼ˆSWAï¼‰çš„çª—å£å¤§å°ã€‚å¦‚æœæœªæŒ‡å®šï¼Œé»˜è®¤ä¸º `4096`ã€‚
        max_window_layers (`int`, *optional*, defaults to 80):
            ä½¿ç”¨å…¨æ³¨æ„åŠ›çš„å±‚æ•°ã€‚å‰ `max_window_layers` å±‚å°†ä½¿ç”¨å…¨æ³¨æ„åŠ›ï¼Œ
            ä¹‹åçš„ä»»ä½•é¢å¤–å±‚å°†ä½¿ç”¨ SWAï¼ˆæ»‘åŠ¨çª—å£æ³¨æ„åŠ›ï¼‰ã€‚
        layer_types (`list`, *optional*):
            æ¯å±‚çš„æ³¨æ„åŠ›æ¨¡å¼ã€‚å¯ä»¥ä¸ºä¸åŒå±‚æŒ‡å®šä¸åŒçš„æ³¨æ„åŠ›ç±»å‹ã€‚
        attention_dropout (`float`, *optional*, defaults to 0.0):
            æ³¨æ„åŠ›æ¦‚ç‡çš„ dropout æ¯”ç‡ã€‚ç”¨äºæ­£åˆ™åŒ–é˜²æ­¢è¿‡æ‹Ÿåˆã€‚
        rope_scaling (`Dict`, *optional*):
            åŒ…å« RoPE åµŒå…¥ç¼©æ”¾é…ç½®çš„å­—å…¸ã€‚æ³¨æ„ï¼šå¦‚æœåº”ç”¨æ–°çš„ rope ç±»å‹å¹¶æœŸæœ›æ¨¡å‹åœ¨æ›´é•¿çš„
            `max_position_embeddings` ä¸Šå·¥ä½œï¼Œå»ºè®®ç›¸åº”åœ°æ›´æ–°æ­¤å€¼ã€‚
            é¢„æœŸå†…å®¹ï¼š
                `rope_type` (`str`):
                    è¦ä½¿ç”¨çš„ RoPE å­å˜ä½“ã€‚å¯ä»¥æ˜¯ ['default', 'linear', 'dynamic', 'yarn', 'longrope',
                    'llama3'] ä¹‹ä¸€ï¼Œå…¶ä¸­ 'default' æ˜¯åŸå§‹çš„ RoPE å®ç°ã€‚
                `factor` (`float`, *optional*):
                    ç”¨äºé™¤ 'default' å¤–çš„æ‰€æœ‰ rope ç±»å‹ã€‚åº”ç”¨äº RoPE åµŒå…¥çš„ç¼©æ”¾å› å­ã€‚
                    åœ¨å¤§å¤šæ•°ç¼©æ”¾ç±»å‹ä¸­ï¼Œå› å­ x å°†ä½¿æ¨¡å‹èƒ½å¤Ÿå¤„ç†é•¿åº¦ä¸º x * åŸå§‹æœ€å¤§é¢„è®­ç»ƒé•¿åº¦çš„åºåˆ—ã€‚
                `original_max_position_embeddings` (`int`, *optional*):
                    ç”¨äº 'dynamic'ã€'longrope' å’Œ 'llama3'ã€‚é¢„è®­ç»ƒæœŸé—´ä½¿ç”¨çš„åŸå§‹æœ€å¤§ä½ç½®åµŒå…¥ã€‚
                `attention_factor` (`float`, *optional*):
                    ç”¨äº 'yarn' å’Œ 'longrope'ã€‚åº”ç”¨äºæ³¨æ„åŠ›è®¡ç®—çš„ç¼©æ”¾å› å­ã€‚
                    å¦‚æœæœªæŒ‡å®šï¼Œé»˜è®¤ä¸ºå®ç°æ¨èçš„å€¼ï¼Œä½¿ç”¨ `factor` å­—æ®µæ¨æ–­å»ºè®®å€¼ã€‚
                `beta_fast` (`float`, *optional*):
                    ä»…ç”¨äº 'yarn'ã€‚åœ¨çº¿æ€§æ–œå¡å‡½æ•°ä¸­è®¾ç½®å¤–æ¨ï¼ˆä»…ï¼‰è¾¹ç•Œçš„å‚æ•°ã€‚å¦‚æœæœªæŒ‡å®šï¼Œé»˜è®¤ä¸º 32ã€‚
                `beta_slow` (`float`, *optional*):
                    ä»…ç”¨äº 'yarn'ã€‚åœ¨çº¿æ€§æ–œå¡å‡½æ•°ä¸­è®¾ç½®æ’å€¼ï¼ˆä»…ï¼‰è¾¹ç•Œçš„å‚æ•°ã€‚å¦‚æœæœªæŒ‡å®šï¼Œé»˜è®¤ä¸º 1ã€‚
                `short_factor` (`list[float]`, *optional*):
                    ä»…ç”¨äº 'longrope'ã€‚åº”ç”¨äºçŸ­ä¸Šä¸‹æ–‡ï¼ˆ< `original_max_position_embeddings`ï¼‰çš„ç¼©æ”¾å› å­ã€‚
                    å¿…é¡»æ˜¯é•¿åº¦ç­‰äºéšè—å¤§å°é™¤ä»¥æ³¨æ„åŠ›å¤´æ•°å†é™¤ä»¥ 2 çš„æ•°å­—åˆ—è¡¨ã€‚
                `long_factor` (`list[float]`, *optional*):
                    ä»…ç”¨äº 'longrope'ã€‚åº”ç”¨äºé•¿ä¸Šä¸‹æ–‡ï¼ˆ< `original_max_position_embeddings`ï¼‰çš„ç¼©æ”¾å› å­ã€‚
                    å¿…é¡»æ˜¯é•¿åº¦ç­‰äºéšè—å¤§å°é™¤ä»¥æ³¨æ„åŠ›å¤´æ•°å†é™¤ä»¥ 2 çš„æ•°å­—åˆ—è¡¨ã€‚
                `low_freq_factor` (`float`, *optional*):
                    ä»…ç”¨äº 'llama3'ã€‚åº”ç”¨äº RoPE ä½é¢‘åˆ†é‡çš„ç¼©æ”¾å› å­ã€‚
                `high_freq_factor` (`float`, *optional*):
                    ä»…ç”¨äº 'llama3'ã€‚åº”ç”¨äº RoPE é«˜é¢‘åˆ†é‡çš„ç¼©æ”¾å› å­ã€‚
        image_token_id (`int`, *optional*):
            ç”¨ä½œå›¾åƒåµŒå…¥å ä½ç¬¦çš„ token ç´¢å¼•ã€‚
        video_token_id (`int`, *optional*):
            ç”¨ä½œè§†é¢‘åµŒå…¥å ä½ç¬¦çš„ token ç´¢å¼•ã€‚

    ```python
    >>> from transformers import Qwen2_5_VLTextModel, Qwen2_5_VLConfig

    >>> # Initializing a Qwen2_5_VL style configuration
    >>> configuration = Qwen2_5_VLConfig()

    >>> # Initializing a model from the Qwen2-VL-7B style configuration
    >>> model = Qwen2_5_VLTextModel(configuration)

    >>> # Accessing the model configuration
    >>> configuration = model.config
    ```"""

    model_type = "qwen2_5_vl_text"
    base_config_key = "text_config"
    keys_to_ignore_at_inference = ["past_key_values"]
    # Default tensor parallel plan for base model `Qwen2_5_VL`
    base_model_tp_plan = {
        "layers.*.self_attn.q_proj": "colwise",
        "layers.*.self_attn.k_proj": "colwise",
        "layers.*.self_attn.v_proj": "colwise",
        "layers.*.self_attn.o_proj": "rowwise",
        "layers.*.mlp.gate_proj": "colwise",
        "layers.*.mlp.up_proj": "colwise",
        "layers.*.mlp.down_proj": "rowwise",
    }
    base_model_pp_plan = {
        "embed_tokens": (["input_ids"], ["inputs_embeds"]),
        "layers": (["hidden_states", "attention_mask"], ["hidden_states"]),
        "norm": (["hidden_states"], ["hidden_states"]),
    }

    def __init__(
        self,
        vocab_size=152064,
        hidden_size=8192,
        intermediate_size=29568,
        num_hidden_layers=80,
        num_attention_heads=64,
        num_key_value_heads=8,
        hidden_act="silu",
        max_position_embeddings=32768,
        initializer_range=0.02,
        rms_norm_eps=1e-05,
        use_cache=True,
        tie_word_embeddings=False,
        rope_theta=1000000.0,
        use_sliding_window=False,
        sliding_window=4096,
        max_window_layers=80,
        layer_types=None,
        attention_dropout=0.0,
        rope_scaling=None,
        image_token_id=None,
        video_token_id=None,
        **kwargs,
    ):
        self.vocab_size = vocab_size
        self.max_position_embeddings = max_position_embeddings
        self.hidden_size = hidden_size
        self.intermediate_size = intermediate_size
        self.num_hidden_layers = num_hidden_layers
        self.num_attention_heads = num_attention_heads
        self.use_sliding_window = use_sliding_window
        self.sliding_window = sliding_window if self.use_sliding_window else None
        self.max_window_layers = max_window_layers

        # for backward compatibility
        if num_key_value_heads is None:
            num_key_value_heads = num_attention_heads

        self.num_key_value_heads = num_key_value_heads
        self.hidden_act = hidden_act
        self.initializer_range = initializer_range
        self.rms_norm_eps = rms_norm_eps
        self.use_cache = use_cache
        self.rope_theta = rope_theta
        self.attention_dropout = attention_dropout
        self.rope_scaling = rope_scaling

        self.layer_types = layer_types
        if self.layer_types is None:
            self.layer_types = [
                "sliding_attention"
                if self.sliding_window is not None and i >= self.max_window_layers
                else "full_attention"
                for i in range(self.num_hidden_layers)
            ]
        layer_type_validation(self.layer_types)

        # Validate the correctness of rotary position embeddings parameters
        # BC: if there is a 'type' field, move it to 'rope_type'.
        # and change type from 'mrope' to 'default' because `mrope` does default RoPE calculations
        # one can set it to "linear"/"dynamic" etc. to have scaled RoPE
        # TODO: @raushan update config in the hub
        if self.rope_scaling is not None and "type" in self.rope_scaling:
            if self.rope_scaling["type"] == "mrope":
                self.rope_scaling["type"] = "default"
            self.rope_scaling["rope_type"] = self.rope_scaling["type"]
        rope_config_validation(self, ignore_keys={"mrope_section"})
        self.image_token_id = image_token_id
        self.video_token_id = video_token_id

        super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)


class Qwen2_5_VLConfig(PretrainedConfig):
    r"""
    Qwen2.5-VL æ¨¡å‹çš„é…ç½®ç±»ã€‚
    
    è¿™æ˜¯ç”¨äºå­˜å‚¨ [`Qwen2_5_VLModel`] é…ç½®çš„é…ç½®ç±»ã€‚å®ƒç”¨äºæ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–
    Qwen2.5-VL æ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äº
    Qwen2.5-VL-7B-Instruct [Qwen/Qwen2-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct) çš„é…ç½®ã€‚

    é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª [`PretrainedConfig`] å¹¶å¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œ
    è¯·é˜…è¯» [`PretrainedConfig`] çš„æ–‡æ¡£ã€‚

    è¯¥é…ç½®ç±»æ•´åˆäº†è§†è§‰ç¼–ç å™¨å’Œæ–‡æœ¬ç¼–ç å™¨çš„é…ç½®ï¼Œæä¾›äº†å®Œæ•´çš„å¤šæ¨¡æ€æ¨¡å‹é…ç½®ç®¡ç†ã€‚
    å®ƒå®šä¹‰äº†æ¨¡å‹æ¶æ„çš„æ‰€æœ‰å…³é”®å‚æ•°ï¼ŒåŒ…æ‹¬ç‰¹æ®Š token çš„ ID å’Œå„ä¸ªç»„ä»¶çš„é…ç½®ã€‚

    Args:
        text_config (`Union[PreTrainedConfig, dict]`, *optional*, defaults to `Qwen2_5_VLTextConfig`):
            æ–‡æœ¬éª¨å¹²ç½‘ç»œçš„é…ç½®å¯¹è±¡æˆ–å­—å…¸ã€‚å®šä¹‰æ–‡æœ¬ç¼–ç å™¨çš„æ‰€æœ‰å‚æ•°ï¼ŒåŒ…æ‹¬è¯æ±‡è¡¨å¤§å°ã€
            éšè—å±‚ç»´åº¦ã€æ³¨æ„åŠ›å¤´æ•°ç­‰ã€‚
        vision_config (`Union[PreTrainedConfig, dict]`,  *optional*, defaults to `Qwen2_5_VLVisionConfig`):
            è§†è§‰éª¨å¹²ç½‘ç»œçš„é…ç½®å¯¹è±¡æˆ–å­—å…¸ã€‚å®šä¹‰è§†è§‰ç¼–ç å™¨çš„æ‰€æœ‰å‚æ•°ï¼ŒåŒ…æ‹¬å›¾åƒå—å¤§å°ã€
            Transformer å±‚æ•°ã€æ³¨æ„åŠ›æœºåˆ¶ç­‰ã€‚
        image_token_id (`int`, *optional*, defaults to 151655):
            ç”¨äºç¼–ç å›¾åƒæç¤ºçš„å›¾åƒ token ç´¢å¼•ã€‚åœ¨å¤„ç†åŒ…å«å›¾åƒçš„è¾“å…¥æ—¶ï¼Œ
            æ­¤ token ç”¨ä½œå›¾åƒå†…å®¹çš„å ä½ç¬¦ã€‚
        video_token_id (`int`, *optional*, defaults to 151656):
            ç”¨äºç¼–ç è§†é¢‘æç¤ºçš„è§†é¢‘ token ç´¢å¼•ã€‚åœ¨å¤„ç†åŒ…å«è§†é¢‘çš„è¾“å…¥æ—¶ï¼Œ
            æ­¤ token ç”¨ä½œè§†é¢‘å†…å®¹çš„å ä½ç¬¦ã€‚

    ```python
    >>> from transformers import Qwen2_5_VLForConditionalGeneration, Qwen2_5_VLConfig

    >>> # åˆå§‹åŒ– Qwen2_5_VL é£æ ¼çš„é…ç½®
    >>> configuration = Qwen2_5_VLConfig()

    >>> # ä» Qwen2.5-VL-7B é£æ ¼é…ç½®åˆå§‹åŒ–æ¨¡å‹
    >>> model = Qwen2_5_VLForConditionalGeneration(configuration)

    >>> # è®¿é—®æ¨¡å‹é…ç½®
    >>> configuration = model.config
    ```"""

    model_type = "qwen2_5_vl"
    sub_configs = {"vision_config": Qwen2_5_VLVisionConfig, "text_config": Qwen2_5_VLTextConfig}
    keys_to_ignore_at_inference = ["past_key_values"]

    def __init__(
        self,
        text_config=None,
        vision_config=None,
        image_token_id=151655,
        video_token_id=151656,
        **kwargs,
    ):
        if isinstance(vision_config, dict):
            self.vision_config = self.sub_configs["vision_config"](**vision_config)
        elif vision_config is None:
            self.vision_config = self.sub_configs["vision_config"]()

        if isinstance(text_config, dict):
            self.text_config = self.sub_configs["text_config"](**text_config)
        elif text_config is None:
            # For BC use all kwargs to init `TextConfig`
            self.text_config = self.sub_configs["text_config"](**kwargs)

        self.image_token_id = image_token_id
        self.video_token_id = video_token_id

        super().__init__(**kwargs)


__all__ = ["Qwen2_5_VLConfig", "Qwen2_5_VLTextConfig"]
