#                🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨
#           This file was automatically generated from src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_qwen2_5_vl.py file directly. One of our CI enforces this.
#                🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨
# coding=utf-8
# Copyright 2025 The Qwen Team and The HuggingFace Inc. team. All rights reserved.
#
# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX
# and OPT implementations in this library. It has been modified from its
# original forms to accommodate minor architectural differences compared
# to GPT-NeoX and OPT used by the Meta AI team that trained the model.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Qwen2.5-VL 模型的处理器模块

本文件定义了 Qwen2.5-VL 多模态大语言模型的处理器类，用于统一处理文本、图像和视频输入。
主要功能包括：

1. **多模态输入处理**：
   - 文本输入的 tokenization
   - 图像输入的预处理和特征提取
   - 视频输入的预处理和特征提取
   - 多模态输入的统一格式化

2. **处理器架构**：
   - Qwen2_5_VLProcessor：主处理器类，整合文本、图像和视频处理器
   - 各种 Kwargs 类：定义不同模态输入的参数配置
   - 特殊 token 处理：管理图像和视频占位符 token

3. **核心处理流程**：
   - 输入验证和格式化
   - 多模态特征提取
   - token 序列生成和对齐
   - 输出格式统一

4. **主要类和方法**：
   - Qwen2_5_VLProcessor：主处理器类
   - __call__：核心处理方法
   - _get_num_multimodal_tokens：计算多模态 token 数量
   - batch_decode/decode：文本解码方法
   - post_process_image_text_to_text：后处理方法

该处理器是 Qwen2.5-VL 模型的关键组件，负责将原始的多模态输入转换为模型可以理解的格式。
"""

from typing import Optional, Union

import numpy as np

from ...feature_extraction_utils import BatchFeature
from ...image_utils import ImageInput
from ...processing_utils import ImagesKwargs, MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack, VideosKwargs
from ...tokenization_utils_base import PreTokenizedInput, TextInput
from ...video_utils import VideoInput


class Qwen2_5_VLVideosProcessorKwargs(VideosKwargs, total=False):
    """
    Qwen2.5-VL 视频处理器的关键字参数类。
    
    继承自 VideosKwargs，用于定义视频处理时的特定参数。
    该类扩展了基础视频处理参数，添加了 Qwen2.5-VL 模型特有的视频处理配置。
    
    Args:
        fps (Union[list[float], float]):
            视频的帧率（每秒帧数）。可以是单个浮点数（应用于所有视频）
            或浮点数列表（每个视频对应一个帧率值）。
            用于计算视频时间网格和时间戳信息。
    """
    fps: Union[list[float], float]


class Qwen2_5_VLImagesKwargs(ImagesKwargs):
    """
    Qwen2.5-VL 图像处理器的关键字参数类。
    
    继承自 ImagesKwargs，用于定义图像处理时的特定参数。
    该类扩展了基础图像处理参数，添加了 Qwen2.5-VL 模型特有的图像处理配置。
    
    Args:
        min_pixels (Optional[int]):
            图像的最小像素数。用于控制图像缩放的下限，确保图像不会被缩放得过小。
        max_pixels (Optional[int]):
            图像的最大像素数。用于控制图像缩放的上限，防止图像过大导致内存问题。
        patch_size (Optional[int]):
            图像块的大小。定义将图像分割成小块时每个块的尺寸，
            影响视觉 Transformer 的输入粒度。
        temporal_patch_size (Optional[int]):
            时间维度的块大小。主要用于视频处理，定义时间轴上的分块大小。
        merge_size (Optional[int]):
            合并块的大小。用于将多个相邻的图像块合并，
            影响最终输入到语言模型的 token 数量。
    """
    min_pixels: Optional[int]
    max_pixels: Optional[int]
    patch_size: Optional[int]
    temporal_patch_size: Optional[int]
    merge_size: Optional[int]


class Qwen2_5_VLProcessorKwargs(ProcessingKwargs, total=False):
    """
    Qwen2.5-VL 处理器的关键字参数类。
    
    继承自 ProcessingKwargs，用于定义多模态处理时的参数配置。
    该类整合了文本、图像和视频处理的所有参数，提供统一的参数管理接口。
    
    Args:
        images_kwargs (Qwen2_5_VLImagesKwargs):
            图像处理相关的参数配置。包含图像预处理、缩放、分块等参数。
        videos_kwargs (Qwen2_5_VLVideosProcessorKwargs):
            视频处理相关的参数配置。包含视频帧率、时间分块等参数。
    
    Attributes:
        _defaults (dict):
            默认参数配置。定义了文本处理的默认行为：
            - padding: False - 默认不进行填充
            - return_mm_token_type_ids: False - 默认不返回多模态 token 类型 ID
    """
    images_kwargs: Qwen2_5_VLImagesKwargs
    videos_kwargs: Qwen2_5_VLVideosProcessorKwargs
    _defaults = {
        "text_kwargs": {
            "padding": False,
            "return_mm_token_type_ids": False,
        },
    }


class Qwen2_5_VLProcessor(ProcessorMixin):
    r"""
    构建 Qwen2.5-VL 处理器，将 Qwen2.5-VL 图像处理器和 Qwen2 分词器封装到单个处理器中。
    
    [`Qwen2_5_VLProcessor`] 提供了 [`Qwen2VLImageProcessor`] 和 [`Qwen2TokenizerFast`] 的所有功能。
    有关更多信息，请参阅 [`~Qwen2_5_VLProcessor.__call__`] 和 [`~Qwen2_5_VLProcessor.decode`]。
    
    该处理器是 Qwen2.5-VL 多模态模型的核心组件，负责：
    1. 统一处理文本、图像和视频输入
    2. 管理特殊 token（图像和视频占位符）
    3. 协调不同模态的预处理流程
    4. 生成模型所需的统一输入格式
    
    Args:
        image_processor ([`Qwen2VLImageProcessor`], *optional*):
            图像处理器，是必需的输入。负责图像的预处理、特征提取和格式化。
        tokenizer ([`Qwen2TokenizerFast`], *optional*):
            分词器，是必需的输入。负责文本的 tokenization 和编码。
        video_processor ([`Qwen2_5_VLVideoProcessor`], *optional*):
            视频处理器，是必需的输入。负责视频的预处理、帧提取和特征化。
        chat_template (`str`, *optional*):
            Jinja 模板，用于将聊天中的消息列表转换为可 tokenize 的字符串。
            支持多轮对话的格式化处理。
    """

    attributes = ["image_processor", "tokenizer", "video_processor"]

    image_processor_class = "AutoImageProcessor"
    video_processor_class = "AutoVideoProcessor"
    tokenizer_class = ("Qwen2Tokenizer", "Qwen2TokenizerFast")

    def __init__(self, image_processor=None, tokenizer=None, video_processor=None, chat_template=None, **kwargs):
        self.image_token = "<|image_pad|>" if not hasattr(tokenizer, "image_token") else tokenizer.image_token
        self.video_token = "<|video_pad|>" if not hasattr(tokenizer, "video_token") else tokenizer.video_token
        self.image_token_id = (
            tokenizer.image_token_id
            if getattr(tokenizer, "image_token_id", None)
            else tokenizer.convert_tokens_to_ids(self.image_token)
        )
        self.video_token_id = (
            tokenizer.video_token_id
            if getattr(tokenizer, "video_token_id", None)
            else tokenizer.convert_tokens_to_ids(self.video_token)
        )
        super().__init__(image_processor, tokenizer, video_processor, chat_template=chat_template)

    def __call__(
        self,
        images: ImageInput = None,
        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,
        videos: VideoInput = None,
        **kwargs: Unpack[Qwen2_5_VLProcessorKwargs],
    ) -> BatchFeature:
        """
        为模型准备一个或多个序列和图像的主要方法。
        
        该方法是 Qwen2.5-VL 处理器的核心接口，负责统一处理多模态输入：
        - 如果 `text` 不为 `None`，将 `text` 和 `kwargs` 参数转发给 Qwen2TokenizerFast 的 [`~Qwen2TokenizerFast.__call__`] 来编码文本
        - 如果 `images` 不为 `None`，将图像参数转发给 Qwen2VLImageProcessor 的 [`~Qwen2VLImageProcessor.__call__`] 来处理图像
        - 如果 `videos` 不为 `None`，将视频参数转发给视频处理器来处理视频
        
        处理流程包括：
        1. 多模态输入的预处理和特征提取
        2. 特殊 token 的插入和管理
        3. 不同模态输入的对齐和统一格式化
        4. 生成模型所需的批量特征

        Args:
            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):
                要准备的图像或图像批次。每个图像可以是 PIL 图像、NumPy 数组或 PyTorch 张量。
                支持通道优先和通道最后两种格式。
            text (`str`, `list[str]`, `list[list[str]]`):
                要编码的序列或序列批次。每个序列可以是字符串或字符串列表（预分词字符串）。
                如果序列以字符串列表形式提供（预分词），必须设置 `is_split_into_words=True`
                （以消除与序列批次的歧义）。
            videos (`np.ndarray`, `torch.Tensor`, `list[np.ndarray]`, `list[torch.Tensor]`):
                要准备的视频或视频批次。每个视频可以是 4D NumPy 数组或 PyTorch 张量，
                或 3D 帧的嵌套列表。支持通道优先和通道最后两种格式。
            return_tensors (`str` or [`~utils.TensorType`], *optional*):
                如果设置，将返回特定框架的张量。可接受的值：
                - `'tf'`: 返回 TensorFlow `tf.constant` 对象
                - `'pt'`: 返回 PyTorch `torch.Tensor` 对象
                - `'np'`: 返回 NumPy `np.ndarray` 对象
                - `'jax'`: 返回 JAX `jnp.ndarray` 对象

        Returns:
            [`BatchFeature`]: 包含以下字段的 [`BatchFeature`]：

            - **input_ids** -- 要输入模型的 token ID 列表。当 `text` 不为 `None` 时返回。
            - **attention_mask** -- 指定模型应关注哪些 token 的索引列表（当 `return_attention_mask=True`
              或 *"attention_mask"* 在 `self.model_input_names` 中且 `text` 不为 `None` 时）。
            - **pixel_values** -- 要输入模型的像素值。当 `images` 不为 `None` 时返回。
            - **pixel_values_videos** -- 要输入模型的视频像素值。当 `videos` 不为 `None` 时返回。
            - **image_grid_thw** -- LLM 中的图像 3D 网格列表。当 `images` 不为 `None` 时返回。
            - **video_grid_thw** -- LLM 中的视频 3D 网格列表。当 `videos` 不为 `None` 时返回。
            - **second_per_grid_ts** -- 每个时间网格的视频秒数列表。当 `videos` 不为 `None` 时返回。
        """
        output_kwargs = self._merge_kwargs(
            Qwen2_5_VLProcessorKwargs,
            tokenizer_init_kwargs=self.tokenizer.init_kwargs,
            **kwargs,
        )

        image_inputs = videos_inputs = {}
        if images is not None:
            image_inputs = self.image_processor(images=images, **output_kwargs["images_kwargs"])
            image_grid_thw = image_inputs["image_grid_thw"]

        if videos is not None:
            fps = output_kwargs["videos_kwargs"].get("fps", 2.0)
            videos_inputs = self.video_processor(videos=videos, **output_kwargs["videos_kwargs"])
            video_grid_thw = videos_inputs["video_grid_thw"]

            if isinstance(fps, (int, float)):
                second_per_grid_ts = [self.video_processor.temporal_patch_size / fps] * len(video_grid_thw)
            elif hasattr(fps, "__len__") and len(fps) == len(video_grid_thw):
                second_per_grid_ts = [self.video_processor.temporal_patch_size / tmp for tmp in fps]
            else:
                raise ValueError(
                    f"The length of fps ({len(fps) if hasattr(fps, '__len__') else fps}) must be equal to the length of video_grid_thw ({len(video_grid_thw)}) or fps should be a single number."
                )
            videos_inputs.update({"second_per_grid_ts": second_per_grid_ts})

        if not isinstance(text, list):
            text = [text]

        text = text.copy()  # below lines change text in-place
        if images is not None:
            merge_length = self.image_processor.merge_size**2
            index = 0
            for i in range(len(text)):
                while self.image_token in text[i]:
                    num_image_tokens = image_grid_thw[index].prod() // merge_length
                    text[i] = text[i].replace(self.image_token, "<|placeholder|>" * num_image_tokens, 1)
                    index += 1
                text[i] = text[i].replace("<|placeholder|>", self.image_token)

        if videos is not None:
            merge_length = self.video_processor.merge_size**2
            index = 0
            for i in range(len(text)):
                while self.video_token in text[i]:
                    num_video_tokens = video_grid_thw[index].prod() // merge_length
                    text[i] = text[i].replace(self.video_token, "<|placeholder|>" * num_video_tokens, 1)
                    index += 1
                text[i] = text[i].replace("<|placeholder|>", self.video_token)

        return_tensors = output_kwargs["text_kwargs"].pop("return_tensors", None)
        return_mm_token_type_ids = output_kwargs["text_kwargs"].pop("return_mm_token_type_ids", None)
        text_inputs = self.tokenizer(text, **output_kwargs["text_kwargs"])
        self._check_special_mm_tokens(text, text_inputs, modalities=["image", "video"])

        if return_mm_token_type_ids:
            array_ids = np.array(text_inputs["input_ids"])
            mm_token_type_ids = np.zeros_like(text_inputs["input_ids"])
            mm_token_type_ids[array_ids == self.image_token_id] = 1
            text_inputs["mm_token_type_ids"] = mm_token_type_ids.tolist()

        return BatchFeature(data={**text_inputs, **image_inputs, **videos_inputs}, tensor_type=return_tensors)

    def _get_num_multimodal_tokens(self, image_sizes=None, video_sizes=None, **kwargs):
        """
        计算给定尺寸的多模态输入所需的占位符 token 数量。
        
        该方法是多模态 token 计算的核心函数，用于：
        1. 根据图像和视频的尺寸计算所需的 token 数量
        2. 考虑图像块大小和合并策略
        3. 为模型输入准备正确的 token 占位符数量
        
        Args:
            image_sizes (`list[list[int]]`, *optional*):
                每个图像的输入尺寸，格式为 (height, width)。
                用于计算图像需要多少个 token 来表示。
            video_sizes (`list[list[int]]`, *optional*):
                每个视频的输入尺寸，格式为 (num_frames, height, width)。
                用于计算视频需要多少个 token 来表示。
                
        Returns:
            `MultiModalData`: 包含每个提供的输入模态的 token 数量以及其他有用数据的
            `MultiModalData` 对象。包括图像 token 数量、视频 token 数量和相关的块信息。
        """

        vision_data = {}
        if image_sizes is not None:
            images_kwargs = Qwen2_5_VLProcessorKwargs._defaults.get("images_kwargs", {})
            images_kwargs.update(kwargs)
            merge_size = images_kwargs.get("merge_size", None) or self.image_processor.merge_size

            num_image_patches = [
                self.image_processor.get_number_of_image_patches(*image_size, images_kwargs)
                for image_size in image_sizes
            ]
            num_image_tokens = [(num_patches // merge_size**2) for num_patches in num_image_patches]
            vision_data.update({"num_image_tokens": num_image_tokens, "num_image_patches": num_image_patches})

        if video_sizes is not None:
            videos_kwargs = Qwen2_5_VLProcessorKwargs._defaults.get("videos_kwargs", {})
            videos_kwargs.update(kwargs)
            num_video_patches = [
                self.video_processor.get_number_of_video_patches(*video_size, videos_kwargs)
                for video_size in video_sizes
            ]
            num_video_tokens = [(num_patches // merge_size**2) for num_patches in num_video_patches]
            vision_data["num_video_tokens"] = num_video_tokens

        return MultiModalData(**vision_data)

    def batch_decode(self, *args, **kwargs):
        """
        批量解码方法，将所有参数转发给 Qwen2TokenizerFast 的 [`~PreTrainedTokenizer.batch_decode`]。
        
        该方法提供了对底层分词器批量解码功能的直接访问，用于将 token ID 序列
        转换回可读的文本。有关更多信息，请参阅该方法的文档字符串。
        """
        return self.tokenizer.batch_decode(*args, **kwargs)

    def decode(self, *args, **kwargs):
        """
        单个解码方法，将所有参数转发给 Qwen2TokenizerFast 的 [`~PreTrainedTokenizer.decode`]。
        
        该方法提供了对底层分词器解码功能的直接访问，用于将单个 token ID 序列
        转换回可读的文本。有关更多信息，请参阅该方法的文档字符串。
        """
        return self.tokenizer.decode(*args, **kwargs)

    def post_process_image_text_to_text(
        self, generated_outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False, **kwargs
    ):
        """
        对模型输出进行后处理以解码文本。
        
        该方法是多模态模型输出的标准后处理接口，将模型生成的 token ID 序列
        转换为最终的文本输出。特别适用于图像到文本的生成任务。

        Args:
            generated_outputs (`torch.Tensor` or `np.ndarray`):
                模型 `generate` 函数的输出。输出应该是形状为 `(batch_size, sequence_length)`
                或 `(sequence_length,)` 的张量。
            skip_special_tokens (`bool`, *optional*, defaults to `True`):
                是否在输出中移除特殊 token。该参数传递给分词器的 `batch_decode` 方法。
            clean_up_tokenization_spaces (`bool`, *optional*, defaults to `False`):
                是否清理分词空格。该参数传递给分词器的 `batch_decode` 方法。
            **kwargs:
                传递给分词器 `batch_decode` 方法的额外参数。

        Returns:
            `list[str]`: 解码后的文本列表。每个元素对应一个输入序列的解码结果。
        """
        return self.tokenizer.batch_decode(
            generated_outputs,
            skip_special_tokens=skip_special_tokens,
            clean_up_tokenization_spaces=clean_up_tokenization_spaces,
            **kwargs,
        )

    @property
    def model_input_names(self):
        """
        获取模型输入名称列表。
        
        该属性整合了分词器和图像处理器的模型输入名称，并添加了 Qwen2.5-VL 特有的输入名称。
        返回的名称列表定义了模型期望接收的所有输入参数名称。
        
        组合逻辑：
        1. 获取分词器的模型输入名称（如 input_ids, attention_mask 等）
        2. 获取图像处理器的模型输入名称（如 pixel_values, image_grid_thw 等）
        3. 合并并去重，保持顺序
        4. 添加视频处理特有的输入名称（second_per_grid_ts）
        
        Returns:
            list[str]: 模型输入名称的完整列表，包含文本、图像和视频处理的所有必要输入。
        """
        tokenizer_input_names = self.tokenizer.model_input_names
        image_processor_input_names = self.image_processor.model_input_names
        names_from_processor = list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))
        return names_from_processor + ["second_per_grid_ts"]


__all__ = ["Qwen2_5_VLProcessor"]
