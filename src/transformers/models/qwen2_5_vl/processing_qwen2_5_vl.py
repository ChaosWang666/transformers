#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨
#           This file was automatically generated from src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_qwen2_5_vl.py file directly. One of our CI enforces this.
#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨
# coding=utf-8
# Copyright 2025 The Qwen Team and The HuggingFace Inc. team. All rights reserved.
#
# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX
# and OPT implementations in this library. It has been modified from its
# original forms to accommodate minor architectural differences compared
# to GPT-NeoX and OPT used by the Meta AI team that trained the model.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Qwen2.5-VL æ¨¡å‹çš„å¤„ç†å™¨æ¨¡å—

æœ¬æ–‡ä»¶å®šä¹‰äº† Qwen2.5-VL å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å¤„ç†å™¨ç±»ï¼Œç”¨äºç»Ÿä¸€å¤„ç†æ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘è¾“å…¥ã€‚
ä¸»è¦åŠŸèƒ½åŒ…æ‹¬ï¼š

1. **å¤šæ¨¡æ€è¾“å…¥å¤„ç†**ï¼š
   - æ–‡æœ¬è¾“å…¥çš„ tokenization
   - å›¾åƒè¾“å…¥çš„é¢„å¤„ç†å’Œç‰¹å¾æå–
   - è§†é¢‘è¾“å…¥çš„é¢„å¤„ç†å’Œç‰¹å¾æå–
   - å¤šæ¨¡æ€è¾“å…¥çš„ç»Ÿä¸€æ ¼å¼åŒ–

2. **å¤„ç†å™¨æ¶æ„**ï¼š
   - Qwen2_5_VLProcessorï¼šä¸»å¤„ç†å™¨ç±»ï¼Œæ•´åˆæ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘å¤„ç†å™¨
   - å„ç§ Kwargs ç±»ï¼šå®šä¹‰ä¸åŒæ¨¡æ€è¾“å…¥çš„å‚æ•°é…ç½®
   - ç‰¹æ®Š token å¤„ç†ï¼šç®¡ç†å›¾åƒå’Œè§†é¢‘å ä½ç¬¦ token

3. **æ ¸å¿ƒå¤„ç†æµç¨‹**ï¼š
   - è¾“å…¥éªŒè¯å’Œæ ¼å¼åŒ–
   - å¤šæ¨¡æ€ç‰¹å¾æå–
   - token åºåˆ—ç”Ÿæˆå’Œå¯¹é½
   - è¾“å‡ºæ ¼å¼ç»Ÿä¸€

4. **ä¸»è¦ç±»å’Œæ–¹æ³•**ï¼š
   - Qwen2_5_VLProcessorï¼šä¸»å¤„ç†å™¨ç±»
   - __call__ï¼šæ ¸å¿ƒå¤„ç†æ–¹æ³•
   - _get_num_multimodal_tokensï¼šè®¡ç®—å¤šæ¨¡æ€ token æ•°é‡
   - batch_decode/decodeï¼šæ–‡æœ¬è§£ç æ–¹æ³•
   - post_process_image_text_to_textï¼šåå¤„ç†æ–¹æ³•

è¯¥å¤„ç†å™¨æ˜¯ Qwen2.5-VL æ¨¡å‹çš„å…³é”®ç»„ä»¶ï¼Œè´Ÿè´£å°†åŸå§‹çš„å¤šæ¨¡æ€è¾“å…¥è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥ç†è§£çš„æ ¼å¼ã€‚
"""

from typing import Optional, Union

import numpy as np

from ...feature_extraction_utils import BatchFeature
from ...image_utils import ImageInput
from ...processing_utils import ImagesKwargs, MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack, VideosKwargs
from ...tokenization_utils_base import PreTokenizedInput, TextInput
from ...video_utils import VideoInput


class Qwen2_5_VLVideosProcessorKwargs(VideosKwargs, total=False):
    """
    Qwen2.5-VL è§†é¢‘å¤„ç†å™¨çš„å…³é”®å­—å‚æ•°ç±»ã€‚
    
    ç»§æ‰¿è‡ª VideosKwargsï¼Œç”¨äºå®šä¹‰è§†é¢‘å¤„ç†æ—¶çš„ç‰¹å®šå‚æ•°ã€‚
    è¯¥ç±»æ‰©å±•äº†åŸºç¡€è§†é¢‘å¤„ç†å‚æ•°ï¼Œæ·»åŠ äº† Qwen2.5-VL æ¨¡å‹ç‰¹æœ‰çš„è§†é¢‘å¤„ç†é…ç½®ã€‚
    
    Args:
        fps (Union[list[float], float]):
            è§†é¢‘çš„å¸§ç‡ï¼ˆæ¯ç§’å¸§æ•°ï¼‰ã€‚å¯ä»¥æ˜¯å•ä¸ªæµ®ç‚¹æ•°ï¼ˆåº”ç”¨äºæ‰€æœ‰è§†é¢‘ï¼‰
            æˆ–æµ®ç‚¹æ•°åˆ—è¡¨ï¼ˆæ¯ä¸ªè§†é¢‘å¯¹åº”ä¸€ä¸ªå¸§ç‡å€¼ï¼‰ã€‚
            ç”¨äºè®¡ç®—è§†é¢‘æ—¶é—´ç½‘æ ¼å’Œæ—¶é—´æˆ³ä¿¡æ¯ã€‚
    """
    fps: Union[list[float], float]


class Qwen2_5_VLImagesKwargs(ImagesKwargs):
    """
    Qwen2.5-VL å›¾åƒå¤„ç†å™¨çš„å…³é”®å­—å‚æ•°ç±»ã€‚
    
    ç»§æ‰¿è‡ª ImagesKwargsï¼Œç”¨äºå®šä¹‰å›¾åƒå¤„ç†æ—¶çš„ç‰¹å®šå‚æ•°ã€‚
    è¯¥ç±»æ‰©å±•äº†åŸºç¡€å›¾åƒå¤„ç†å‚æ•°ï¼Œæ·»åŠ äº† Qwen2.5-VL æ¨¡å‹ç‰¹æœ‰çš„å›¾åƒå¤„ç†é…ç½®ã€‚
    
    Args:
        min_pixels (Optional[int]):
            å›¾åƒçš„æœ€å°åƒç´ æ•°ã€‚ç”¨äºæ§åˆ¶å›¾åƒç¼©æ”¾çš„ä¸‹é™ï¼Œç¡®ä¿å›¾åƒä¸ä¼šè¢«ç¼©æ”¾å¾—è¿‡å°ã€‚
        max_pixels (Optional[int]):
            å›¾åƒçš„æœ€å¤§åƒç´ æ•°ã€‚ç”¨äºæ§åˆ¶å›¾åƒç¼©æ”¾çš„ä¸Šé™ï¼Œé˜²æ­¢å›¾åƒè¿‡å¤§å¯¼è‡´å†…å­˜é—®é¢˜ã€‚
        patch_size (Optional[int]):
            å›¾åƒå—çš„å¤§å°ã€‚å®šä¹‰å°†å›¾åƒåˆ†å‰²æˆå°å—æ—¶æ¯ä¸ªå—çš„å°ºå¯¸ï¼Œ
            å½±å“è§†è§‰ Transformer çš„è¾“å…¥ç²’åº¦ã€‚
        temporal_patch_size (Optional[int]):
            æ—¶é—´ç»´åº¦çš„å—å¤§å°ã€‚ä¸»è¦ç”¨äºè§†é¢‘å¤„ç†ï¼Œå®šä¹‰æ—¶é—´è½´ä¸Šçš„åˆ†å—å¤§å°ã€‚
        merge_size (Optional[int]):
            åˆå¹¶å—çš„å¤§å°ã€‚ç”¨äºå°†å¤šä¸ªç›¸é‚»çš„å›¾åƒå—åˆå¹¶ï¼Œ
            å½±å“æœ€ç»ˆè¾“å…¥åˆ°è¯­è¨€æ¨¡å‹çš„ token æ•°é‡ã€‚
    """
    min_pixels: Optional[int]
    max_pixels: Optional[int]
    patch_size: Optional[int]
    temporal_patch_size: Optional[int]
    merge_size: Optional[int]


class Qwen2_5_VLProcessorKwargs(ProcessingKwargs, total=False):
    """
    Qwen2.5-VL å¤„ç†å™¨çš„å…³é”®å­—å‚æ•°ç±»ã€‚
    
    ç»§æ‰¿è‡ª ProcessingKwargsï¼Œç”¨äºå®šä¹‰å¤šæ¨¡æ€å¤„ç†æ—¶çš„å‚æ•°é…ç½®ã€‚
    è¯¥ç±»æ•´åˆäº†æ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘å¤„ç†çš„æ‰€æœ‰å‚æ•°ï¼Œæä¾›ç»Ÿä¸€çš„å‚æ•°ç®¡ç†æ¥å£ã€‚
    
    Args:
        images_kwargs (Qwen2_5_VLImagesKwargs):
            å›¾åƒå¤„ç†ç›¸å…³çš„å‚æ•°é…ç½®ã€‚åŒ…å«å›¾åƒé¢„å¤„ç†ã€ç¼©æ”¾ã€åˆ†å—ç­‰å‚æ•°ã€‚
        videos_kwargs (Qwen2_5_VLVideosProcessorKwargs):
            è§†é¢‘å¤„ç†ç›¸å…³çš„å‚æ•°é…ç½®ã€‚åŒ…å«è§†é¢‘å¸§ç‡ã€æ—¶é—´åˆ†å—ç­‰å‚æ•°ã€‚
    
    Attributes:
        _defaults (dict):
            é»˜è®¤å‚æ•°é…ç½®ã€‚å®šä¹‰äº†æ–‡æœ¬å¤„ç†çš„é»˜è®¤è¡Œä¸ºï¼š
            - padding: False - é»˜è®¤ä¸è¿›è¡Œå¡«å……
            - return_mm_token_type_ids: False - é»˜è®¤ä¸è¿”å›å¤šæ¨¡æ€ token ç±»å‹ ID
    """
    images_kwargs: Qwen2_5_VLImagesKwargs
    videos_kwargs: Qwen2_5_VLVideosProcessorKwargs
    _defaults = {
        "text_kwargs": {
            "padding": False,
            "return_mm_token_type_ids": False,
        },
    }


class Qwen2_5_VLProcessor(ProcessorMixin):
    r"""
    æ„å»º Qwen2.5-VL å¤„ç†å™¨ï¼Œå°† Qwen2.5-VL å›¾åƒå¤„ç†å™¨å’Œ Qwen2 åˆ†è¯å™¨å°è£…åˆ°å•ä¸ªå¤„ç†å™¨ä¸­ã€‚
    
    [`Qwen2_5_VLProcessor`] æä¾›äº† [`Qwen2VLImageProcessor`] å’Œ [`Qwen2TokenizerFast`] çš„æ‰€æœ‰åŠŸèƒ½ã€‚
    æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜… [`~Qwen2_5_VLProcessor.__call__`] å’Œ [`~Qwen2_5_VLProcessor.decode`]ã€‚
    
    è¯¥å¤„ç†å™¨æ˜¯ Qwen2.5-VL å¤šæ¨¡æ€æ¨¡å‹çš„æ ¸å¿ƒç»„ä»¶ï¼Œè´Ÿè´£ï¼š
    1. ç»Ÿä¸€å¤„ç†æ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘è¾“å…¥
    2. ç®¡ç†ç‰¹æ®Š tokenï¼ˆå›¾åƒå’Œè§†é¢‘å ä½ç¬¦ï¼‰
    3. åè°ƒä¸åŒæ¨¡æ€çš„é¢„å¤„ç†æµç¨‹
    4. ç”Ÿæˆæ¨¡å‹æ‰€éœ€çš„ç»Ÿä¸€è¾“å…¥æ ¼å¼
    
    Args:
        image_processor ([`Qwen2VLImageProcessor`], *optional*):
            å›¾åƒå¤„ç†å™¨ï¼Œæ˜¯å¿…éœ€çš„è¾“å…¥ã€‚è´Ÿè´£å›¾åƒçš„é¢„å¤„ç†ã€ç‰¹å¾æå–å’Œæ ¼å¼åŒ–ã€‚
        tokenizer ([`Qwen2TokenizerFast`], *optional*):
            åˆ†è¯å™¨ï¼Œæ˜¯å¿…éœ€çš„è¾“å…¥ã€‚è´Ÿè´£æ–‡æœ¬çš„ tokenization å’Œç¼–ç ã€‚
        video_processor ([`Qwen2_5_VLVideoProcessor`], *optional*):
            è§†é¢‘å¤„ç†å™¨ï¼Œæ˜¯å¿…éœ€çš„è¾“å…¥ã€‚è´Ÿè´£è§†é¢‘çš„é¢„å¤„ç†ã€å¸§æå–å’Œç‰¹å¾åŒ–ã€‚
        chat_template (`str`, *optional*):
            Jinja æ¨¡æ¿ï¼Œç”¨äºå°†èŠå¤©ä¸­çš„æ¶ˆæ¯åˆ—è¡¨è½¬æ¢ä¸ºå¯ tokenize çš„å­—ç¬¦ä¸²ã€‚
            æ”¯æŒå¤šè½®å¯¹è¯çš„æ ¼å¼åŒ–å¤„ç†ã€‚
    """

    attributes = ["image_processor", "tokenizer", "video_processor"]

    image_processor_class = "AutoImageProcessor"
    video_processor_class = "AutoVideoProcessor"
    tokenizer_class = ("Qwen2Tokenizer", "Qwen2TokenizerFast")

    def __init__(self, image_processor=None, tokenizer=None, video_processor=None, chat_template=None, **kwargs):
        self.image_token = "<|image_pad|>" if not hasattr(tokenizer, "image_token") else tokenizer.image_token
        self.video_token = "<|video_pad|>" if not hasattr(tokenizer, "video_token") else tokenizer.video_token
        self.image_token_id = (
            tokenizer.image_token_id
            if getattr(tokenizer, "image_token_id", None)
            else tokenizer.convert_tokens_to_ids(self.image_token)
        )
        self.video_token_id = (
            tokenizer.video_token_id
            if getattr(tokenizer, "video_token_id", None)
            else tokenizer.convert_tokens_to_ids(self.video_token)
        )
        super().__init__(image_processor, tokenizer, video_processor, chat_template=chat_template)

    def __call__(
        self,
        images: ImageInput = None,
        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,
        videos: VideoInput = None,
        **kwargs: Unpack[Qwen2_5_VLProcessorKwargs],
    ) -> BatchFeature:
        """
        ä¸ºæ¨¡å‹å‡†å¤‡ä¸€ä¸ªæˆ–å¤šä¸ªåºåˆ—å’Œå›¾åƒçš„ä¸»è¦æ–¹æ³•ã€‚
        
        è¯¥æ–¹æ³•æ˜¯ Qwen2.5-VL å¤„ç†å™¨çš„æ ¸å¿ƒæ¥å£ï¼Œè´Ÿè´£ç»Ÿä¸€å¤„ç†å¤šæ¨¡æ€è¾“å…¥ï¼š
        - å¦‚æœ `text` ä¸ä¸º `None`ï¼Œå°† `text` å’Œ `kwargs` å‚æ•°è½¬å‘ç»™ Qwen2TokenizerFast çš„ [`~Qwen2TokenizerFast.__call__`] æ¥ç¼–ç æ–‡æœ¬
        - å¦‚æœ `images` ä¸ä¸º `None`ï¼Œå°†å›¾åƒå‚æ•°è½¬å‘ç»™ Qwen2VLImageProcessor çš„ [`~Qwen2VLImageProcessor.__call__`] æ¥å¤„ç†å›¾åƒ
        - å¦‚æœ `videos` ä¸ä¸º `None`ï¼Œå°†è§†é¢‘å‚æ•°è½¬å‘ç»™è§†é¢‘å¤„ç†å™¨æ¥å¤„ç†è§†é¢‘
        
        å¤„ç†æµç¨‹åŒ…æ‹¬ï¼š
        1. å¤šæ¨¡æ€è¾“å…¥çš„é¢„å¤„ç†å’Œç‰¹å¾æå–
        2. ç‰¹æ®Š token çš„æ’å…¥å’Œç®¡ç†
        3. ä¸åŒæ¨¡æ€è¾“å…¥çš„å¯¹é½å’Œç»Ÿä¸€æ ¼å¼åŒ–
        4. ç”Ÿæˆæ¨¡å‹æ‰€éœ€çš„æ‰¹é‡ç‰¹å¾

        Args:
            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):
                è¦å‡†å¤‡çš„å›¾åƒæˆ–å›¾åƒæ‰¹æ¬¡ã€‚æ¯ä¸ªå›¾åƒå¯ä»¥æ˜¯ PIL å›¾åƒã€NumPy æ•°ç»„æˆ– PyTorch å¼ é‡ã€‚
                æ”¯æŒé€šé“ä¼˜å…ˆå’Œé€šé“æœ€åä¸¤ç§æ ¼å¼ã€‚
            text (`str`, `list[str]`, `list[list[str]]`):
                è¦ç¼–ç çš„åºåˆ—æˆ–åºåˆ—æ‰¹æ¬¡ã€‚æ¯ä¸ªåºåˆ—å¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„åˆ†è¯å­—ç¬¦ä¸²ï¼‰ã€‚
                å¦‚æœåºåˆ—ä»¥å­—ç¬¦ä¸²åˆ—è¡¨å½¢å¼æä¾›ï¼ˆé¢„åˆ†è¯ï¼‰ï¼Œå¿…é¡»è®¾ç½® `is_split_into_words=True`
                ï¼ˆä»¥æ¶ˆé™¤ä¸åºåˆ—æ‰¹æ¬¡çš„æ­§ä¹‰ï¼‰ã€‚
            videos (`np.ndarray`, `torch.Tensor`, `list[np.ndarray]`, `list[torch.Tensor]`):
                è¦å‡†å¤‡çš„è§†é¢‘æˆ–è§†é¢‘æ‰¹æ¬¡ã€‚æ¯ä¸ªè§†é¢‘å¯ä»¥æ˜¯ 4D NumPy æ•°ç»„æˆ– PyTorch å¼ é‡ï¼Œ
                æˆ– 3D å¸§çš„åµŒå¥—åˆ—è¡¨ã€‚æ”¯æŒé€šé“ä¼˜å…ˆå’Œé€šé“æœ€åä¸¤ç§æ ¼å¼ã€‚
            return_tensors (`str` or [`~utils.TensorType`], *optional*):
                å¦‚æœè®¾ç½®ï¼Œå°†è¿”å›ç‰¹å®šæ¡†æ¶çš„å¼ é‡ã€‚å¯æ¥å—çš„å€¼ï¼š
                - `'tf'`: è¿”å› TensorFlow `tf.constant` å¯¹è±¡
                - `'pt'`: è¿”å› PyTorch `torch.Tensor` å¯¹è±¡
                - `'np'`: è¿”å› NumPy `np.ndarray` å¯¹è±¡
                - `'jax'`: è¿”å› JAX `jnp.ndarray` å¯¹è±¡

        Returns:
            [`BatchFeature`]: åŒ…å«ä»¥ä¸‹å­—æ®µçš„ [`BatchFeature`]ï¼š

            - **input_ids** -- è¦è¾“å…¥æ¨¡å‹çš„ token ID åˆ—è¡¨ã€‚å½“ `text` ä¸ä¸º `None` æ—¶è¿”å›ã€‚
            - **attention_mask** -- æŒ‡å®šæ¨¡å‹åº”å…³æ³¨å“ªäº› token çš„ç´¢å¼•åˆ—è¡¨ï¼ˆå½“ `return_attention_mask=True`
              æˆ– *"attention_mask"* åœ¨ `self.model_input_names` ä¸­ä¸” `text` ä¸ä¸º `None` æ—¶ï¼‰ã€‚
            - **pixel_values** -- è¦è¾“å…¥æ¨¡å‹çš„åƒç´ å€¼ã€‚å½“ `images` ä¸ä¸º `None` æ—¶è¿”å›ã€‚
            - **pixel_values_videos** -- è¦è¾“å…¥æ¨¡å‹çš„è§†é¢‘åƒç´ å€¼ã€‚å½“ `videos` ä¸ä¸º `None` æ—¶è¿”å›ã€‚
            - **image_grid_thw** -- LLM ä¸­çš„å›¾åƒ 3D ç½‘æ ¼åˆ—è¡¨ã€‚å½“ `images` ä¸ä¸º `None` æ—¶è¿”å›ã€‚
            - **video_grid_thw** -- LLM ä¸­çš„è§†é¢‘ 3D ç½‘æ ¼åˆ—è¡¨ã€‚å½“ `videos` ä¸ä¸º `None` æ—¶è¿”å›ã€‚
            - **second_per_grid_ts** -- æ¯ä¸ªæ—¶é—´ç½‘æ ¼çš„è§†é¢‘ç§’æ•°åˆ—è¡¨ã€‚å½“ `videos` ä¸ä¸º `None` æ—¶è¿”å›ã€‚
        """
        output_kwargs = self._merge_kwargs(
            Qwen2_5_VLProcessorKwargs,
            tokenizer_init_kwargs=self.tokenizer.init_kwargs,
            **kwargs,
        )

        image_inputs = videos_inputs = {}
        if images is not None:
            image_inputs = self.image_processor(images=images, **output_kwargs["images_kwargs"])
            image_grid_thw = image_inputs["image_grid_thw"]

        if videos is not None:
            fps = output_kwargs["videos_kwargs"].get("fps", 2.0)
            videos_inputs = self.video_processor(videos=videos, **output_kwargs["videos_kwargs"])
            video_grid_thw = videos_inputs["video_grid_thw"]

            if isinstance(fps, (int, float)):
                second_per_grid_ts = [self.video_processor.temporal_patch_size / fps] * len(video_grid_thw)
            elif hasattr(fps, "__len__") and len(fps) == len(video_grid_thw):
                second_per_grid_ts = [self.video_processor.temporal_patch_size / tmp for tmp in fps]
            else:
                raise ValueError(
                    f"The length of fps ({len(fps) if hasattr(fps, '__len__') else fps}) must be equal to the length of video_grid_thw ({len(video_grid_thw)}) or fps should be a single number."
                )
            videos_inputs.update({"second_per_grid_ts": second_per_grid_ts})

        if not isinstance(text, list):
            text = [text]

        text = text.copy()  # below lines change text in-place
        if images is not None:
            merge_length = self.image_processor.merge_size**2
            index = 0
            for i in range(len(text)):
                while self.image_token in text[i]:
                    num_image_tokens = image_grid_thw[index].prod() // merge_length
                    text[i] = text[i].replace(self.image_token, "<|placeholder|>" * num_image_tokens, 1)
                    index += 1
                text[i] = text[i].replace("<|placeholder|>", self.image_token)

        if videos is not None:
            merge_length = self.video_processor.merge_size**2
            index = 0
            for i in range(len(text)):
                while self.video_token in text[i]:
                    num_video_tokens = video_grid_thw[index].prod() // merge_length
                    text[i] = text[i].replace(self.video_token, "<|placeholder|>" * num_video_tokens, 1)
                    index += 1
                text[i] = text[i].replace("<|placeholder|>", self.video_token)

        return_tensors = output_kwargs["text_kwargs"].pop("return_tensors", None)
        return_mm_token_type_ids = output_kwargs["text_kwargs"].pop("return_mm_token_type_ids", None)
        text_inputs = self.tokenizer(text, **output_kwargs["text_kwargs"])
        self._check_special_mm_tokens(text, text_inputs, modalities=["image", "video"])

        if return_mm_token_type_ids:
            array_ids = np.array(text_inputs["input_ids"])
            mm_token_type_ids = np.zeros_like(text_inputs["input_ids"])
            mm_token_type_ids[array_ids == self.image_token_id] = 1
            text_inputs["mm_token_type_ids"] = mm_token_type_ids.tolist()

        return BatchFeature(data={**text_inputs, **image_inputs, **videos_inputs}, tensor_type=return_tensors)

    def _get_num_multimodal_tokens(self, image_sizes=None, video_sizes=None, **kwargs):
        """
        è®¡ç®—ç»™å®šå°ºå¯¸çš„å¤šæ¨¡æ€è¾“å…¥æ‰€éœ€çš„å ä½ç¬¦ token æ•°é‡ã€‚
        
        è¯¥æ–¹æ³•æ˜¯å¤šæ¨¡æ€ token è®¡ç®—çš„æ ¸å¿ƒå‡½æ•°ï¼Œç”¨äºï¼š
        1. æ ¹æ®å›¾åƒå’Œè§†é¢‘çš„å°ºå¯¸è®¡ç®—æ‰€éœ€çš„ token æ•°é‡
        2. è€ƒè™‘å›¾åƒå—å¤§å°å’Œåˆå¹¶ç­–ç•¥
        3. ä¸ºæ¨¡å‹è¾“å…¥å‡†å¤‡æ­£ç¡®çš„ token å ä½ç¬¦æ•°é‡
        
        Args:
            image_sizes (`list[list[int]]`, *optional*):
                æ¯ä¸ªå›¾åƒçš„è¾“å…¥å°ºå¯¸ï¼Œæ ¼å¼ä¸º (height, width)ã€‚
                ç”¨äºè®¡ç®—å›¾åƒéœ€è¦å¤šå°‘ä¸ª token æ¥è¡¨ç¤ºã€‚
            video_sizes (`list[list[int]]`, *optional*):
                æ¯ä¸ªè§†é¢‘çš„è¾“å…¥å°ºå¯¸ï¼Œæ ¼å¼ä¸º (num_frames, height, width)ã€‚
                ç”¨äºè®¡ç®—è§†é¢‘éœ€è¦å¤šå°‘ä¸ª token æ¥è¡¨ç¤ºã€‚
                
        Returns:
            `MultiModalData`: åŒ…å«æ¯ä¸ªæä¾›çš„è¾“å…¥æ¨¡æ€çš„ token æ•°é‡ä»¥åŠå…¶ä»–æœ‰ç”¨æ•°æ®çš„
            `MultiModalData` å¯¹è±¡ã€‚åŒ…æ‹¬å›¾åƒ token æ•°é‡ã€è§†é¢‘ token æ•°é‡å’Œç›¸å…³çš„å—ä¿¡æ¯ã€‚
        """

        vision_data = {}
        if image_sizes is not None:
            images_kwargs = Qwen2_5_VLProcessorKwargs._defaults.get("images_kwargs", {})
            images_kwargs.update(kwargs)
            merge_size = images_kwargs.get("merge_size", None) or self.image_processor.merge_size

            num_image_patches = [
                self.image_processor.get_number_of_image_patches(*image_size, images_kwargs)
                for image_size in image_sizes
            ]
            num_image_tokens = [(num_patches // merge_size**2) for num_patches in num_image_patches]
            vision_data.update({"num_image_tokens": num_image_tokens, "num_image_patches": num_image_patches})

        if video_sizes is not None:
            videos_kwargs = Qwen2_5_VLProcessorKwargs._defaults.get("videos_kwargs", {})
            videos_kwargs.update(kwargs)
            num_video_patches = [
                self.video_processor.get_number_of_video_patches(*video_size, videos_kwargs)
                for video_size in video_sizes
            ]
            num_video_tokens = [(num_patches // merge_size**2) for num_patches in num_video_patches]
            vision_data["num_video_tokens"] = num_video_tokens

        return MultiModalData(**vision_data)

    def batch_decode(self, *args, **kwargs):
        """
        æ‰¹é‡è§£ç æ–¹æ³•ï¼Œå°†æ‰€æœ‰å‚æ•°è½¬å‘ç»™ Qwen2TokenizerFast çš„ [`~PreTrainedTokenizer.batch_decode`]ã€‚
        
        è¯¥æ–¹æ³•æä¾›äº†å¯¹åº•å±‚åˆ†è¯å™¨æ‰¹é‡è§£ç åŠŸèƒ½çš„ç›´æ¥è®¿é—®ï¼Œç”¨äºå°† token ID åºåˆ—
        è½¬æ¢å›å¯è¯»çš„æ–‡æœ¬ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…è¯¥æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²ã€‚
        """
        return self.tokenizer.batch_decode(*args, **kwargs)

    def decode(self, *args, **kwargs):
        """
        å•ä¸ªè§£ç æ–¹æ³•ï¼Œå°†æ‰€æœ‰å‚æ•°è½¬å‘ç»™ Qwen2TokenizerFast çš„ [`~PreTrainedTokenizer.decode`]ã€‚
        
        è¯¥æ–¹æ³•æä¾›äº†å¯¹åº•å±‚åˆ†è¯å™¨è§£ç åŠŸèƒ½çš„ç›´æ¥è®¿é—®ï¼Œç”¨äºå°†å•ä¸ª token ID åºåˆ—
        è½¬æ¢å›å¯è¯»çš„æ–‡æœ¬ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…è¯¥æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²ã€‚
        """
        return self.tokenizer.decode(*args, **kwargs)

    def post_process_image_text_to_text(
        self, generated_outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False, **kwargs
    ):
        """
        å¯¹æ¨¡å‹è¾“å‡ºè¿›è¡Œåå¤„ç†ä»¥è§£ç æ–‡æœ¬ã€‚
        
        è¯¥æ–¹æ³•æ˜¯å¤šæ¨¡æ€æ¨¡å‹è¾“å‡ºçš„æ ‡å‡†åå¤„ç†æ¥å£ï¼Œå°†æ¨¡å‹ç”Ÿæˆçš„ token ID åºåˆ—
        è½¬æ¢ä¸ºæœ€ç»ˆçš„æ–‡æœ¬è¾“å‡ºã€‚ç‰¹åˆ«é€‚ç”¨äºå›¾åƒåˆ°æ–‡æœ¬çš„ç”Ÿæˆä»»åŠ¡ã€‚

        Args:
            generated_outputs (`torch.Tensor` or `np.ndarray`):
                æ¨¡å‹ `generate` å‡½æ•°çš„è¾“å‡ºã€‚è¾“å‡ºåº”è¯¥æ˜¯å½¢çŠ¶ä¸º `(batch_size, sequence_length)`
                æˆ– `(sequence_length,)` çš„å¼ é‡ã€‚
            skip_special_tokens (`bool`, *optional*, defaults to `True`):
                æ˜¯å¦åœ¨è¾“å‡ºä¸­ç§»é™¤ç‰¹æ®Š tokenã€‚è¯¥å‚æ•°ä¼ é€’ç»™åˆ†è¯å™¨çš„ `batch_decode` æ–¹æ³•ã€‚
            clean_up_tokenization_spaces (`bool`, *optional*, defaults to `False`):
                æ˜¯å¦æ¸…ç†åˆ†è¯ç©ºæ ¼ã€‚è¯¥å‚æ•°ä¼ é€’ç»™åˆ†è¯å™¨çš„ `batch_decode` æ–¹æ³•ã€‚
            **kwargs:
                ä¼ é€’ç»™åˆ†è¯å™¨ `batch_decode` æ–¹æ³•çš„é¢å¤–å‚æ•°ã€‚

        Returns:
            `list[str]`: è§£ç åçš„æ–‡æœ¬åˆ—è¡¨ã€‚æ¯ä¸ªå…ƒç´ å¯¹åº”ä¸€ä¸ªè¾“å…¥åºåˆ—çš„è§£ç ç»“æœã€‚
        """
        return self.tokenizer.batch_decode(
            generated_outputs,
            skip_special_tokens=skip_special_tokens,
            clean_up_tokenization_spaces=clean_up_tokenization_spaces,
            **kwargs,
        )

    @property
    def model_input_names(self):
        """
        è·å–æ¨¡å‹è¾“å…¥åç§°åˆ—è¡¨ã€‚
        
        è¯¥å±æ€§æ•´åˆäº†åˆ†è¯å™¨å’Œå›¾åƒå¤„ç†å™¨çš„æ¨¡å‹è¾“å…¥åç§°ï¼Œå¹¶æ·»åŠ äº† Qwen2.5-VL ç‰¹æœ‰çš„è¾“å…¥åç§°ã€‚
        è¿”å›çš„åç§°åˆ—è¡¨å®šä¹‰äº†æ¨¡å‹æœŸæœ›æ¥æ”¶çš„æ‰€æœ‰è¾“å…¥å‚æ•°åç§°ã€‚
        
        ç»„åˆé€»è¾‘ï¼š
        1. è·å–åˆ†è¯å™¨çš„æ¨¡å‹è¾“å…¥åç§°ï¼ˆå¦‚ input_ids, attention_mask ç­‰ï¼‰
        2. è·å–å›¾åƒå¤„ç†å™¨çš„æ¨¡å‹è¾“å…¥åç§°ï¼ˆå¦‚ pixel_values, image_grid_thw ç­‰ï¼‰
        3. åˆå¹¶å¹¶å»é‡ï¼Œä¿æŒé¡ºåº
        4. æ·»åŠ è§†é¢‘å¤„ç†ç‰¹æœ‰çš„è¾“å…¥åç§°ï¼ˆsecond_per_grid_tsï¼‰
        
        Returns:
            list[str]: æ¨¡å‹è¾“å…¥åç§°çš„å®Œæ•´åˆ—è¡¨ï¼ŒåŒ…å«æ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘å¤„ç†çš„æ‰€æœ‰å¿…è¦è¾“å…¥ã€‚
        """
        tokenizer_input_names = self.tokenizer.model_input_names
        image_processor_input_names = self.image_processor.model_input_names
        names_from_processor = list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))
        return names_from_processor + ["second_per_grid_ts"]


__all__ = ["Qwen2_5_VLProcessor"]
